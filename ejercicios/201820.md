
# Corte 1

### 1

### 2

### 3

### 4

### 5

* Escriba un código para muestrear la distribución exponencial inversa ((1/lambda)(1/x**2)exp(-lambda/x)) con el algoritmo de Metropolis-Hastings.
Prepare una gráfica con el histograma de la cadena de Markov para verificar que tiene la respuesta correcta.

### 6 (Para Calificar)

* Escriba un codigo para ajustar datos bidimensionales a una parabola usando 
estadistica Bayesiana y el algoritmo de Metropolis Hastings.

# Corte 2

### 7 

* Escriba u codigo para ajustar datos bidimensionales a una parabola usando 
estadistica Bayesiana y el algoritmo de Monte Carlo Hamiltoniano.

### 8 (Para Calificar)

* A través de fotos aéreas se tienen medidas del alcance de un
  lanzador de proyectiles. Se tienen cinco valores (en metros): 880,
  795, 782, 976, 178. Todas las mediciones tienen una incertidumbre de
  5 metros. Los valores diferentes del alcance se deben a diferentes
  ángulos de lanzamiento, la velocidad inicial es aproximadamente la
  misma. Utilice el teorema de Bayes y el método de
  Metropolis-Hastings para encontrar la distribución de probabilidad
  de la velocidad inicial dados los datos observacionales. La
  respuesta debe estar implementada dentro de un archivo .py. El
  código debe producir una única gráfica de la distribución de
  probabilidad pedidad donde en el título se anotan la velocidad que
  maximiza la probabilidad, el valor medio y la desviación estándar.  
  
  
 ### 9
 
 ### 10
 
 ### 11
 
 Utilizando los datos en `data/fitting.txt` calcule la evidencia bayesiana para M modelos diferentes, donde cada modelo es un polinomio de orden m. Prepare una grafica de la evidencia como funcion del orden del polinomio. El prior para los coeficientes del polinomio debe ser uniforme entre -1 y 1, y  el orden de los polinomios debe variar entre 0<=m<=20.
 
 ### 12 
 
 Reproduzca el panel derecho la Figura 2.9 de ISLR (solamente las curvas de test y training) utilizando los datos de `fitting.txt` y métodos MCMC para encontrar los mejor parámetros de los modelos polinomiales de grado cero hasta diez.
 
 # Corte 3
 
 ### 13

Ejercicios del notebook `06.Regresion_lineal/regresion_lineal.ipynb`
 
 ### 14
 
Ejercicios del notebook `07.CrossValidation/bootstrap.ipynb`

### 15

Implemente (i.e. no reutilice funciones de scikit-learn):

1. Leave-One-Out Cross-Validation

2. k-Fold Cross-Validation

para calcular el error cuadrático medio como función del orden polinomial de un ajuste de `Price` como funcion de `Horsepower`. Intente con polinomios de orden 1 hasta 10. Compare esas dos graficas con el Criterio de informacion Bayesiana como funcion del orden del polinomio.

La solución debe estar en un notebook que asuma que el archivo de datos se encuentra en el mismo directorio que el notebook.

### 16 (Para Calificar)

Usando como target a 'Price' y como variables ['MPG.city', 'MPG.highway', 'EngineSize',    'Horsepower', 'RPM', 'Rev.per.mile',   'Fuel.tank.capacity', 'Passengers', 'Length',   'Wheelbase', 'Width', 'Turn.circle', 'Weight'], el objetivo de este ejercicio es elegir el conjunto de variables que mayor influencia tiene sobre el target (asumiendo un modelo lineal). Para esto van a utilizar regularizacion con LASSO y Leave-One-Out cross-validation. El objetivo es reproducir el equivalente de la Figura 6.12 y reportar la combinacion de variables que más influencia tiene. Pueden utilizar sklearn.linear_model.Lasso() y sklearn.model_selection.LeaveOneOut().

# Cuarto corte

### 17 

(primera parte)

Usando los siguientes datos: 

https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Default.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Default.csv

Reproduzca las siguientes Figuras del texto guia:

  - Figura 4.1.
  - Panel derecho Figura 4.2.
  - Figura 4.3.

Haga los ajustes usando scikit-learn http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

(segunda parte)

Divida los datos anteriores en training y test. Calcule la matriz de confusión para esos dos conjuntos de datos. Diría que estos resultados son buenos para predecir Default?

Usando los siguientes datos: 
https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Smarket.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Smarket.csv

Dividida también los datos en training y test, en training van todos los datos hasta el año 2004, en test van los demás.
Use la regresión logística para predecir `Direction` en función de las demás variables.
Calcule la matriz de confusión para esos dos conjuntos de datos. Diría que estos resultados son buenos para predecir `Direction`?

## 18

(primera parte)

a. Utilice Linear Discriminant Analysis (LDA)para reproducir las Figuras 4.7 y 4.8 del texto guia.

b. Utilizando los datos de `Default` construya una curva de precision-recall con los resultados del modelo regresión logística (http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html). *Escriba su propia función. No utilice la de sklearn*

c. Utilizando el siguiente dataset 

https://vincentarelbundock.github.io/Rdatasets/doc/MASS/Boston.html
https://vincentarelbundock.github.io/Rdatasets/csv/MASS/Boston.csv

encuentre los mejores predictores para saber si un suburbio tiene una rata de crimen mayor o menor a la media.
Utilice LDA y regresion logistica junto a curvas ROC y Precision-Recall para justificar su respuesta.

(segunda parte)

d. Utilizando el siguiente dataset

https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Auto.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Auto.csv

repita los mismos pasos del punto c. para predecir si un carro tiene `mpg` mayor o menor a la media.

## 19

(Primera parte)

a. Partiendo del siguiente dataset
https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/Hitters.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Hitters.csv

utilice funciones de sklearn para reproducir la Figura 8.5 del texto guia.

b. Partiendo del siguiente dataset
https://archive.ics.uci.edu/ml/datasets/Heart+Disease

utilice funciones de sklearn para reproducir el panel izquierdo de la Figura 8.6.

(Segunda Parte)

c. Partiendo del siguiente dataset
https://vincentarelbundock.github.io/Rdatasets/doc/ISLR/OJ.html
https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/OJ.csv

separe los datos en training+validation y test para encontrar las características
del árbol que mejor predice `Purchase`. Tome en cuenta todos los predictores salvo `Store7`.
Utilice las curvas ROC y Precision-Recall como el criterio para encontrar el mejor árbol.

# 20 (Para Calificar - bono de 10%)

(Primera parte)

a) Reproduzca la Figura 8.8 del texto guia.

b) Reproduzca la Figura 8.10 del texto guia. Utilice el dataset `OJ`. 

(Segunda parte)

c) Utilice el data set `XX` para encontrar cual de los siguientes modelos logra predecir mejor el target `YY`:
  - Logistic regresion.
  - Linear discriminant analysis.
  - Classification Tree.
  - Random Forest.
  Haga explicito el criterio que utiliza para comparar los modelos.

# Quinto corte

### 21

(Primera parte)


a)  Sin utilizar funciones de sklearn, utilice el data set `OJ` para encontrar la recta en el espacio de parametros `LoyalCH`-`PriceDiff` que mejor logra dividir `Purchase` en dos clases. 

b) Resuelva el mismo problema anterior pero utilizando Linear Support Vector Classifier de sklearn http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC para encontrar la mejor rectar de separación.

c) Para cada uno de los dos casos anteriores prepare una gráfica como la Figura 9.8 para mostrar los resultados de los dos clasificadores.

(Segunda parte)

d) Utilice el dataset `Cars93.csv` para crear una variable binaria para los
carros que tienen `MPG.city` mayor/menor que la media.
Utilizando `Fuel.tank.capacity`, `Horsepower`, `Length`, `Rev.per.mile`, `Turn.circle`, `Weight` como predictores encuentre (con y sin sklearn) el mejor 
hiperplano que separa los carros en `MPG.city` mayor/menor que la media. 


### 22

(primera parte)

a) utilice el dataset OJ para predecir purchase. para esto utilice SVM con:
 - kernel lineal, encontrando el mejor parametro C usando cross validation.
 - kernel radial  encontrando el mejor parametro gamma usando cross validation.
 
 (segunda parte)
 
 b) Utilice el dataset `spam7` para predecir `yesno`. Para esto utilice SVM con:
 
 - kernel lineal (encontrando el mejor parametro C usando cross validation.)
 - kernel radial (encontrando el mejor parametro gamma usando cross validation.)
 
 con Precision, Recall, TF1 como los criterios de seleccion.

https://vincentarelbundock.github.io/Rdatasets/datasets.html
 
### 23

(primera parte)

a. Reproducir la Figura 10.1 del texto guia.

b. Reproducir la Figura 10.4 del texto guia.

El dataset se puede bajar de esta pagina https://vincentarelbundock.github.io/Rdatasets/datasets.html

(segunda parte)
 
 c. Utilizando el dataset `Cars93` y las variables `'MPG.highway',
       'Cylinders', 'EngineSize', 'Horsepower', 'RPM', 'Rev.per.mile', 'Fuel.tank.capacity', 'Length',
       'Wheelbase', 'Width', 'Turn.circle', 'Rear.seat.room', 'Luggage.room',
       'Weight'` encuentre las componentes principales que expresan el 80% de la varianza.
 
### 24 (Para Calificar)

(primera parte)

a. Encuentre el número óptimo del clusters en el dataset `dengue` usando como features `'humid', 'temp' , 'h10pix', 'trees' , 'Xmin', 'Ymin'`
    El dataset set puede bajar de esta pagina https://vincentarelbundock.github.io/Rdatasets/datasets.html
    Utilice http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html para encontrar los clusters.
(segunda parte)

(segunda parte)

b. Utilizando el siguiente dataset https://www.kaggle.com/xvivancos/transactions-from-a-bakery encuentre patrones de compras a partir de métodos PCA y k-mean clustering.

# Sexto corte

### 25 

(primera parte)

La respuesta a esta parte debe estar en un repositorio en github llamado `NombreApellido_Ejercicio25`

 a) Escriba un programa en C o C++ (`sample.c`) que al ejecutarse como `./sample N mu sigma`, donde `N` es un entero, genere `N` puntos de 
una distribución gaussiana centrada en `mu` y desviación estándar `sigma`. El código debe escribir los resultados en un archivo de nombre
`sample.dat`. Internamente los `N` puntos de deben almacenar primero en un array (puntero) de tamaño `N` antes de ser escritos al archivo.
**No utilice funciones de libreria que ya devuelvan los números con distribución gaussiana.**

b) Escriba un programa en python que lea los resultados de `sample.dat`, haga un histograma normalizado de los datos y lo compare con
la distribución gaussiana analítica con parámetros `mu` y `sigma` estimados a partir de los datos mismos. El programa debe producir
la gráfica `sample.pdf`

c) Escriba un makefile que automatice de manera adecuada las tareas de los puntos a) y b).

